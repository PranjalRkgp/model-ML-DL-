{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nmH1RBqIp-Iv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mba8p3NWFzn2"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C:\\\\Users\\\\redmi\\\\OneDrive\\\\Desktop\\\\INVICTA\\\\invicta-2022-level-2\\\\inv2_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "4TW05LQjH9oQ",
    "outputId": "6cf0698e-71f1-447c-f83f-5650ee2468e0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917</td>\n",
       "      <td>20</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>...</td>\n",
       "      <td>249.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>754</td>\n",
       "      <td>17</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>181.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1009</td>\n",
       "      <td>23</td>\n",
       "      <td>193.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>...</td>\n",
       "      <td>197.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1327</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1711</td>\n",
       "      <td>28</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>679</td>\n",
       "      <td>15</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>772</td>\n",
       "      <td>18</td>\n",
       "      <td>117.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>...</td>\n",
       "      <td>230.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>682</td>\n",
       "      <td>15</td>\n",
       "      <td>168.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>1136</td>\n",
       "      <td>25</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>272</td>\n",
       "      <td>7</td>\n",
       "      <td>133.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>...</td>\n",
       "      <td>111.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 786 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  Label      0      1      2      3      4      5      6      7  \\\n",
       "0      917     20  255.0  255.0  171.0  149.0  155.0  153.0  156.0  168.0   \n",
       "1      754     17  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0   \n",
       "2     1009     23  193.0  193.0  191.0  191.0  189.0  189.0  190.0  191.0   \n",
       "3     1327     31    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4     1711     28  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0   \n",
       "...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "1195   679     15  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0   \n",
       "1196   772     18  117.0  144.0  153.0  145.0  141.0   95.0  137.0  167.0   \n",
       "1197   682     15  168.0  175.0  177.0  175.0  172.0  162.0  101.0   26.0   \n",
       "1198  1136     25  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0   \n",
       "1199   272      7  133.0  136.0  137.0  139.0  141.0  144.0  146.0  147.0   \n",
       "\n",
       "      ...    774    775    776    777    778    779    780    781    782  \\\n",
       "0     ...  249.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0   \n",
       "1     ...  181.0  246.0  255.0  254.0  255.0  255.0  255.0  255.0  255.0   \n",
       "2     ...  197.0  198.0  200.0  197.0  201.0  201.0  202.0  202.0  202.0   \n",
       "3     ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4     ...   94.0   94.0   94.0   95.0  107.0  111.0  107.0  108.0  113.0   \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "1195  ...  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0   \n",
       "1196  ...  230.0  230.0  216.0  193.0  172.0  154.0  177.0  194.0  206.0   \n",
       "1197  ...   41.0   44.0   44.0   48.0   49.0   57.0   53.0   60.0   63.0   \n",
       "1198  ...  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0   \n",
       "1199  ...  111.0   98.0   80.0   73.0   74.0   73.0   68.0   59.0   56.0   \n",
       "\n",
       "        783  \n",
       "0     255.0  \n",
       "1     255.0  \n",
       "2     205.0  \n",
       "3       0.0  \n",
       "4     104.0  \n",
       "...     ...  \n",
       "1195  255.0  \n",
       "1196  204.0  \n",
       "1197   61.0  \n",
       "1198  255.0  \n",
       "1199   51.0  \n",
       "\n",
       "[1200 rows x 786 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28    132\n",
       "34     60\n",
       "33     49\n",
       "18     43\n",
       "25     43\n",
       "15     40\n",
       "29     39\n",
       "7      38\n",
       "12     37\n",
       "3      36\n",
       "23     35\n",
       "24     35\n",
       "27     35\n",
       "14     34\n",
       "2      33\n",
       "17     33\n",
       "11     32\n",
       "19     32\n",
       "6      31\n",
       "26     30\n",
       "10     30\n",
       "32     29\n",
       "9      27\n",
       "13     27\n",
       "20     26\n",
       "0      25\n",
       "8      24\n",
       "16     24\n",
       "31     24\n",
       "30     22\n",
       "5      22\n",
       "4      20\n",
       "21     19\n",
       "22     18\n",
       "1      16\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wP8YvpPnIGFn",
    "outputId": "6e1348fd-f5c0-4743-b565-ff2f76bb1496"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 917.,   20.,  255., ...,  255.,  255.,  255.],\n",
       "       [ 754.,   17.,  255., ...,  255.,  255.,  255.],\n",
       "       [1009.,   23.,  193., ...,  202.,  202.,  205.],\n",
       "       ...,\n",
       "       [ 682.,   15.,  168., ...,   60.,   63.,   61.],\n",
       "       [1136.,   25.,  255., ...,  255.,  255.,  255.],\n",
       "       [ 272.,    7.,  133., ...,   59.,   56.,   51.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "QSCj-0aOIPJy",
    "outputId": "e4e12ff3-a8ea-41c8-a961-4018e906fdf0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>...</td>\n",
       "      <td>249.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>...</td>\n",
       "      <td>181.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>193.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>...</td>\n",
       "      <td>197.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>253.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>34.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>95.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>199.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>...</td>\n",
       "      <td>120.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>167.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3      4      5      6      7      8      9  \\\n",
       "0    255.0  255.0  171.0  149.0  155.0  153.0  156.0  168.0  151.0  153.0   \n",
       "1    255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  209.0  170.0   \n",
       "2    193.0  193.0  191.0  191.0  189.0  189.0  190.0  191.0  190.0  191.0   \n",
       "3      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4    255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "995  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  254.0  255.0   \n",
       "996    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "997   34.0   33.0   32.0   31.0   33.0   32.0   33.0   34.0   34.0   34.0   \n",
       "998   95.0  100.0  108.0  116.0  101.0   95.0  106.0   96.0   87.0  108.0   \n",
       "999  199.0  200.0  202.0  202.0  203.0  203.0  203.0  203.0  205.0  205.0   \n",
       "\n",
       "     ...    774    775    776    777    778    779    780    781    782    783  \n",
       "0    ...  249.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  \n",
       "1    ...  181.0  246.0  255.0  254.0  255.0  255.0  255.0  255.0  255.0  255.0  \n",
       "2    ...  197.0  198.0  200.0  197.0  201.0  201.0  202.0  202.0  202.0  205.0  \n",
       "3    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4    ...   94.0   94.0   94.0   95.0  107.0  111.0  107.0  108.0  113.0  104.0  \n",
       "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "995  ...  253.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  \n",
       "996  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "997  ...   42.0   49.0   54.0   53.0   51.0   48.0   49.0   49.0   51.0   53.0  \n",
       "998  ...   83.0   79.0   78.0   85.0   90.0   78.0   79.0   70.0   72.0   88.0  \n",
       "999  ...  120.0  116.0  120.0  117.0  135.0  172.0  171.0  171.0  172.0  167.0  \n",
       "\n",
       "[1000 rows x 784 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df.drop(['Id','Label'],axis=1)\n",
    "\n",
    "X=X[0:1000]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "RDvrlpR7Jx4c",
    "outputId": "e7097a6d-325a-4820-8794-c9d80b8e8eb2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0    20\n",
       "1    17\n",
       "2    23\n",
       "3    31\n",
       "4    28\n",
       "..   ..\n",
       "995  28\n",
       "996  28\n",
       "997  12\n",
       "998  18\n",
       "999  10\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=pd.DataFrame(zip(df.Label))\n",
    "Y=Y[0:1000]\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "IKmQdgkGQK2l"
   },
   "outputs": [],
   "source": [
    "# tf.random.set_seed(1234) # for consistent results\n",
    "# model = Sequential(\n",
    "#     [               \n",
    "#         ### START CODE HERE ###\n",
    "#         tf.keras.Input(shape=(784,)),\n",
    "# #         Dense(units=784,activation='relu'),\n",
    "# #         Dense(units=800,activation='relu'),\n",
    "# #         Dense(units=500,activation='relu'),\n",
    "# #         Dense(units=300,activation='relu'),\n",
    "# #         Dense(units=100,activation='relu'),\n",
    "#         Dense(units=5000,activation='relu'),\n",
    "#         Dense(units=300,activation='relu'),\n",
    "# #         Dense(units=9,activation='relu'),\n",
    "#         Dense(units=3,activation='linear')\n",
    "\n",
    "#         ### END CODE HERE ### \n",
    "#     ], name = \"my_model\" \n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkPSdpzPQeYa",
    "outputId": "68db63f7-9d96-41bf-eb96-35bb615c9bcf"
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "fPpG0yfRQkX7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = models.Sequential([\n",
    "    layers.Conv2D(filters=64, kernel_size=(2,2), activation='relu', input_shape=(28,28,1)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    \n",
    "    layers.Conv2D(filters=64, kernel_size=(2,2), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    \n",
    "    layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    \n",
    "#     layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
    "#     layers.MaxPooling2D((2,2)),\n",
    "    \n",
    "#     layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu'),\n",
    "#     layers.MaxPooling2D((2,2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "#     layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(35, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "32/32 [==============================] - 5s 38ms/step - loss: 4.4413 - accuracy: 0.0710\n",
      "Epoch 2/300\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 3.5013 - accuracy: 0.1120\n",
      "Epoch 3/300\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 3.4870 - accuracy: 0.1120\n",
      "Epoch 4/300\n",
      "32/32 [==============================] - 1s 43ms/step - loss: 3.4384 - accuracy: 0.1190\n",
      "Epoch 5/300\n",
      "32/32 [==============================] - 1s 42ms/step - loss: 3.3963 - accuracy: 0.1350\n",
      "Epoch 6/300\n",
      "32/32 [==============================] - 1s 43ms/step - loss: 3.3589 - accuracy: 0.1430\n",
      "Epoch 7/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 3.3225 - accuracy: 0.1450\n",
      "Epoch 8/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 3.3016 - accuracy: 0.1440\n",
      "Epoch 9/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 3.3072 - accuracy: 0.1470\n",
      "Epoch 10/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 3.2355 - accuracy: 0.1530\n",
      "Epoch 11/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 3.2082 - accuracy: 0.1590\n",
      "Epoch 12/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 3.1596 - accuracy: 0.1740\n",
      "Epoch 13/300\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 3.1256 - accuracy: 0.1710\n",
      "Epoch 14/300\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 3.1248 - accuracy: 0.1720\n",
      "Epoch 15/300\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 3.0606 - accuracy: 0.1880\n",
      "Epoch 16/300\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 3.0350 - accuracy: 0.1860\n",
      "Epoch 17/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 3.0165 - accuracy: 0.1880\n",
      "Epoch 18/300\n",
      "32/32 [==============================] - 1s 43ms/step - loss: 3.0064 - accuracy: 0.1840\n",
      "Epoch 19/300\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 3.0579 - accuracy: 0.1760\n",
      "Epoch 20/300\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 2.9412 - accuracy: 0.2010\n",
      "Epoch 21/300\n",
      "32/32 [==============================] - 1s 43ms/step - loss: 2.9066 - accuracy: 0.2130\n",
      "Epoch 22/300\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 2.8864 - accuracy: 0.2130\n",
      "Epoch 23/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 2.8724 - accuracy: 0.2130\n",
      "Epoch 24/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 2.8600 - accuracy: 0.2190\n",
      "Epoch 25/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 2.8336 - accuracy: 0.2180\n",
      "Epoch 26/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 2.7903 - accuracy: 0.2250\n",
      "Epoch 27/300\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 2.7989 - accuracy: 0.2370\n",
      "Epoch 28/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 2.7896 - accuracy: 0.2300\n",
      "Epoch 29/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 2.7679 - accuracy: 0.2380\n",
      "Epoch 30/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 2.7208 - accuracy: 0.2480\n",
      "Epoch 31/300\n",
      "32/32 [==============================] - 1s 44ms/step - loss: 2.7141 - accuracy: 0.2520\n",
      "Epoch 32/300\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 2.6852 - accuracy: 0.2560\n",
      "Epoch 33/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 2.6906 - accuracy: 0.2510\n",
      "Epoch 34/300\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 2.6935 - accuracy: 0.2560\n",
      "Epoch 35/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 2.6818 - accuracy: 0.2590\n",
      "Epoch 36/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 2.6032 - accuracy: 0.2810\n",
      "Epoch 37/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 2.6335 - accuracy: 0.2740\n",
      "Epoch 38/300\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 2.5805 - accuracy: 0.2860\n",
      "Epoch 39/300\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 2.5592 - accuracy: 0.2960\n",
      "Epoch 40/300\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 2.5416 - accuracy: 0.2850\n",
      "Epoch 41/300\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 2.5122 - accuracy: 0.2950\n",
      "Epoch 42/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 2.5194 - accuracy: 0.3050\n",
      "Epoch 43/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 2.5168 - accuracy: 0.3060\n",
      "Epoch 44/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 2.4906 - accuracy: 0.3070\n",
      "Epoch 45/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 2.5051 - accuracy: 0.2960\n",
      "Epoch 46/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 2.5321 - accuracy: 0.2960\n",
      "Epoch 47/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 2.4377 - accuracy: 0.3230\n",
      "Epoch 48/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 2.3874 - accuracy: 0.3270\n",
      "Epoch 49/300\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 2.4112 - accuracy: 0.3290\n",
      "Epoch 50/300\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 2.3733 - accuracy: 0.3290\n",
      "Epoch 51/300\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 2.3669 - accuracy: 0.3440\n",
      "Epoch 52/300\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 2.3428 - accuracy: 0.3530\n",
      "Epoch 53/300\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 2.2981 - accuracy: 0.3680\n",
      "Epoch 54/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 2.2800 - accuracy: 0.3630\n",
      "Epoch 55/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 2.2444 - accuracy: 0.3660\n",
      "Epoch 56/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 2.2746 - accuracy: 0.3620\n",
      "Epoch 57/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 2.2639 - accuracy: 0.3690\n",
      "Epoch 58/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 2.2713 - accuracy: 0.3640\n",
      "Epoch 59/300\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 2.2071 - accuracy: 0.3810\n",
      "Epoch 60/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 2.1902 - accuracy: 0.3810\n",
      "Epoch 61/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 2.1743 - accuracy: 0.3870\n",
      "Epoch 62/300\n",
      "32/32 [==============================] - 1s 42ms/step - loss: 2.2367 - accuracy: 0.3630\n",
      "Epoch 63/300\n",
      "32/32 [==============================] - 1s 42ms/step - loss: 2.1933 - accuracy: 0.3820\n",
      "Epoch 64/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 2.1449 - accuracy: 0.3980\n",
      "Epoch 65/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 2.1637 - accuracy: 0.3900\n",
      "Epoch 66/300\n",
      "32/32 [==============================] - 1s 42ms/step - loss: 2.1115 - accuracy: 0.4010\n",
      "Epoch 67/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 2.0968 - accuracy: 0.4090\n",
      "Epoch 68/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 2.1069 - accuracy: 0.4050\n",
      "Epoch 69/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 2.1088 - accuracy: 0.4030\n",
      "Epoch 70/300\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 2.0417 - accuracy: 0.4150\n",
      "Epoch 71/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 2.0677 - accuracy: 0.4180\n",
      "Epoch 72/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 1.9984 - accuracy: 0.4360\n",
      "Epoch 73/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 1.9893 - accuracy: 0.4210\n",
      "Epoch 74/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 1.9624 - accuracy: 0.4320\n",
      "Epoch 75/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 1.9204 - accuracy: 0.4560\n",
      "Epoch 76/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 1.8908 - accuracy: 0.4560\n",
      "Epoch 77/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 1.8705 - accuracy: 0.4750\n",
      "Epoch 78/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 1.9177 - accuracy: 0.4460\n",
      "Epoch 79/300\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 1.9680 - accuracy: 0.4380\n",
      "Epoch 80/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 1.9503 - accuracy: 0.4420\n",
      "Epoch 81/300\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 1.8914 - accuracy: 0.4520\n",
      "Epoch 82/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 59ms/step - loss: 1.8688 - accuracy: 0.4640\n",
      "Epoch 83/300\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 1.8576 - accuracy: 0.4720\n",
      "Epoch 84/300\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 1.8184 - accuracy: 0.4760\n",
      "Epoch 85/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 1.8441 - accuracy: 0.4630\n",
      "Epoch 86/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 1.9650 - accuracy: 0.4570\n",
      "Epoch 87/300\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 1.9333 - accuracy: 0.4330\n",
      "Epoch 88/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 1.9048 - accuracy: 0.4410\n",
      "Epoch 89/300\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 1.8591 - accuracy: 0.4760\n",
      "Epoch 90/300\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 1.7445 - accuracy: 0.4980\n",
      "Epoch 91/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 1.6940 - accuracy: 0.5120\n",
      "Epoch 92/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 1.6748 - accuracy: 0.5140\n",
      "Epoch 93/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 1.7324 - accuracy: 0.5100\n",
      "Epoch 94/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 1.7817 - accuracy: 0.4880\n",
      "Epoch 95/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 1.6988 - accuracy: 0.5030\n",
      "Epoch 96/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 1.6729 - accuracy: 0.5170\n",
      "Epoch 97/300\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 1.6382 - accuracy: 0.5240\n",
      "Epoch 98/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 1.6332 - accuracy: 0.5260\n",
      "Epoch 99/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 1.7100 - accuracy: 0.5030\n",
      "Epoch 100/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 1.6226 - accuracy: 0.5320\n",
      "Epoch 101/300\n",
      "32/32 [==============================] - 2s 62ms/step - loss: 1.5815 - accuracy: 0.5450\n",
      "Epoch 102/300\n",
      "32/32 [==============================] - 2s 52ms/step - loss: 1.5833 - accuracy: 0.5380\n",
      "Epoch 103/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 1.5592 - accuracy: 0.5490\n",
      "Epoch 104/300\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 1.5537 - accuracy: 0.5420\n",
      "Epoch 105/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.5375 - accuracy: 0.5480\n",
      "Epoch 106/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 1.5186 - accuracy: 0.5550\n",
      "Epoch 107/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.5339 - accuracy: 0.5590\n",
      "Epoch 108/300\n",
      "32/32 [==============================] - 1s 44ms/step - loss: 1.5449 - accuracy: 0.5500\n",
      "Epoch 109/300\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 1.5080 - accuracy: 0.5660\n",
      "Epoch 110/300\n",
      "32/32 [==============================] - 1s 44ms/step - loss: 1.5694 - accuracy: 0.5370\n",
      "Epoch 111/300\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 1.6414 - accuracy: 0.5300\n",
      "Epoch 112/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 1.6507 - accuracy: 0.5180\n",
      "Epoch 113/300\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 1.6472 - accuracy: 0.5100\n",
      "Epoch 114/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 1.7684 - accuracy: 0.4780\n",
      "Epoch 115/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.6606 - accuracy: 0.5000\n",
      "Epoch 116/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.5454 - accuracy: 0.5420\n",
      "Epoch 117/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.4585 - accuracy: 0.5740\n",
      "Epoch 118/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.4161 - accuracy: 0.5860\n",
      "Epoch 119/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.3769 - accuracy: 0.6000\n",
      "Epoch 120/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.3792 - accuracy: 0.5940\n",
      "Epoch 121/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.3750 - accuracy: 0.5950\n",
      "Epoch 122/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.4537 - accuracy: 0.5680\n",
      "Epoch 123/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 1.6130 - accuracy: 0.5330\n",
      "Epoch 124/300\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 1.4850 - accuracy: 0.5600\n",
      "Epoch 125/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 1.4192 - accuracy: 0.5840\n",
      "Epoch 126/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 1.3543 - accuracy: 0.6030\n",
      "Epoch 127/300\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 1.3470 - accuracy: 0.6060\n",
      "Epoch 128/300\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 1.3285 - accuracy: 0.6020\n",
      "Epoch 129/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.3092 - accuracy: 0.6100\n",
      "Epoch 130/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.3116 - accuracy: 0.6100\n",
      "Epoch 131/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.2918 - accuracy: 0.6170\n",
      "Epoch 132/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.3291 - accuracy: 0.5970\n",
      "Epoch 133/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 1.3276 - accuracy: 0.5970\n",
      "Epoch 134/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.3558 - accuracy: 0.5980\n",
      "Epoch 135/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.3733 - accuracy: 0.5880\n",
      "Epoch 136/300\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 1.3261 - accuracy: 0.5960\n",
      "Epoch 137/300\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 1.3582 - accuracy: 0.5830\n",
      "Epoch 138/300\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 1.3346 - accuracy: 0.5900\n",
      "Epoch 139/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 1.4062 - accuracy: 0.5840\n",
      "Epoch 140/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 1.5639 - accuracy: 0.5490\n",
      "Epoch 141/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.4504 - accuracy: 0.5600\n",
      "Epoch 142/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.3854 - accuracy: 0.5900\n",
      "Epoch 143/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.2808 - accuracy: 0.6250\n",
      "Epoch 144/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.2682 - accuracy: 0.6340\n",
      "Epoch 145/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.2394 - accuracy: 0.6390\n",
      "Epoch 146/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.2687 - accuracy: 0.6170\n",
      "Epoch 147/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.2570 - accuracy: 0.6220\n",
      "Epoch 148/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.2278 - accuracy: 0.6400\n",
      "Epoch 149/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 1.1867 - accuracy: 0.6450\n",
      "Epoch 150/300\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 1.1836 - accuracy: 0.6440\n",
      "Epoch 151/300\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 1.1547 - accuracy: 0.6570\n",
      "Epoch 152/300\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 1.1410 - accuracy: 0.6480\n",
      "Epoch 153/300\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 1.1473 - accuracy: 0.6500\n",
      "Epoch 154/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.1758 - accuracy: 0.6490\n",
      "Epoch 155/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.2489 - accuracy: 0.6220\n",
      "Epoch 156/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.2646 - accuracy: 0.6160\n",
      "Epoch 157/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.1944 - accuracy: 0.6420\n",
      "Epoch 158/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.2348 - accuracy: 0.6340\n",
      "Epoch 159/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.1447 - accuracy: 0.6530\n",
      "Epoch 160/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.1267 - accuracy: 0.6610\n",
      "Epoch 161/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.1432 - accuracy: 0.6600\n",
      "Epoch 162/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 38ms/step - loss: 1.4024 - accuracy: 0.5920\n",
      "Epoch 163/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 1.3938 - accuracy: 0.5720\n",
      "Epoch 164/300\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 1.7128 - accuracy: 0.5190\n",
      "Epoch 165/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 1.5790 - accuracy: 0.5460\n",
      "Epoch 166/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.3732 - accuracy: 0.6020\n",
      "Epoch 167/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.3539 - accuracy: 0.6090\n",
      "Epoch 168/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.2062 - accuracy: 0.6290\n",
      "Epoch 169/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.1264 - accuracy: 0.6560\n",
      "Epoch 170/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.1768 - accuracy: 0.6530\n",
      "Epoch 171/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.1068 - accuracy: 0.6600\n",
      "Epoch 172/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.0820 - accuracy: 0.6740\n",
      "Epoch 173/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.0470 - accuracy: 0.6810\n",
      "Epoch 174/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.0441 - accuracy: 0.6790\n",
      "Epoch 175/300\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 1.0324 - accuracy: 0.6880\n",
      "Epoch 176/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 1.0385 - accuracy: 0.6830\n",
      "Epoch 177/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 1.0389 - accuracy: 0.6870\n",
      "Epoch 178/300\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 1.0245 - accuracy: 0.6880\n",
      "Epoch 179/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 1.0244 - accuracy: 0.6930\n",
      "Epoch 180/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 1.0022 - accuracy: 0.6980\n",
      "Epoch 181/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.0020 - accuracy: 0.6890\n",
      "Epoch 182/300\n",
      "32/32 [==============================] - 1s 42ms/step - loss: 1.0383 - accuracy: 0.6810\n",
      "Epoch 183/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 1.1204 - accuracy: 0.6670\n",
      "Epoch 184/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.2564 - accuracy: 0.6300\n",
      "Epoch 185/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.3759 - accuracy: 0.5970\n",
      "Epoch 186/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 1.2468 - accuracy: 0.6190\n",
      "Epoch 187/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 1.2581 - accuracy: 0.6280\n",
      "Epoch 188/300\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 1.2648 - accuracy: 0.6070\n",
      "Epoch 189/300\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 1.2655 - accuracy: 0.6330\n",
      "Epoch 190/300\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 1.1181 - accuracy: 0.6600\n",
      "Epoch 191/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 1.1209 - accuracy: 0.6530\n",
      "Epoch 192/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 1.0709 - accuracy: 0.6760\n",
      "Epoch 193/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 1.0072 - accuracy: 0.6940\n",
      "Epoch 194/300\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.9847 - accuracy: 0.7000\n",
      "Epoch 195/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 0.9733 - accuracy: 0.7010\n",
      "Epoch 196/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.9840 - accuracy: 0.6990\n",
      "Epoch 197/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 0.9643 - accuracy: 0.7110\n",
      "Epoch 198/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 0.9629 - accuracy: 0.7090\n",
      "Epoch 199/300\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 0.9413 - accuracy: 0.7140\n",
      "Epoch 200/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 0.9432 - accuracy: 0.7150\n",
      "Epoch 201/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 0.9491 - accuracy: 0.7040\n",
      "Epoch 202/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 0.9310 - accuracy: 0.7080\n",
      "Epoch 203/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 0.9375 - accuracy: 0.7110\n",
      "Epoch 204/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 0.9300 - accuracy: 0.7130\n",
      "Epoch 205/300\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.9312 - accuracy: 0.7140\n",
      "Epoch 206/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 0.9354 - accuracy: 0.7170\n",
      "Epoch 207/300\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 0.9326 - accuracy: 0.7150\n",
      "Epoch 208/300\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 0.9338 - accuracy: 0.7060\n",
      "Epoch 209/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 0.9268 - accuracy: 0.7080\n",
      "Epoch 210/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 0.9124 - accuracy: 0.7220\n",
      "Epoch 211/300\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.9334 - accuracy: 0.7110\n",
      "Epoch 212/300\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 0.9917 - accuracy: 0.6980\n",
      "Epoch 213/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 1.0006 - accuracy: 0.6810\n",
      "Epoch 214/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 1.0232 - accuracy: 0.6750\n",
      "Epoch 215/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 1.2655 - accuracy: 0.6170\n",
      "Epoch 216/300\n",
      "32/32 [==============================] - 2s 52ms/step - loss: 1.2892 - accuracy: 0.6100\n",
      "Epoch 217/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 1.3743 - accuracy: 0.6010\n",
      "Epoch 218/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 1.4426 - accuracy: 0.5920\n",
      "Epoch 219/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 1.2912 - accuracy: 0.6020\n",
      "Epoch 220/300\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 1.2561 - accuracy: 0.6190\n",
      "Epoch 221/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 1.0826 - accuracy: 0.6610\n",
      "Epoch 222/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 0.9718 - accuracy: 0.7020\n",
      "Epoch 223/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 0.9608 - accuracy: 0.7030\n",
      "Epoch 224/300\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 0.9371 - accuracy: 0.7110\n",
      "Epoch 225/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.9050 - accuracy: 0.7210\n",
      "Epoch 226/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.8819 - accuracy: 0.7330\n",
      "Epoch 227/300\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.8749 - accuracy: 0.7230\n",
      "Epoch 228/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 0.8998 - accuracy: 0.7260\n",
      "Epoch 229/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 0.9287 - accuracy: 0.7160\n",
      "Epoch 230/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 0.8895 - accuracy: 0.7320\n",
      "Epoch 231/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 0.8811 - accuracy: 0.7310\n",
      "Epoch 232/300\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.8582 - accuracy: 0.7270\n",
      "Epoch 233/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 0.9080 - accuracy: 0.7290\n",
      "Epoch 234/300\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.9254 - accuracy: 0.7160\n",
      "Epoch 235/300\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 0.9548 - accuracy: 0.7050\n",
      "Epoch 236/300\n",
      "32/32 [==============================] - 2s 52ms/step - loss: 0.9715 - accuracy: 0.7110\n",
      "Epoch 237/300\n",
      "32/32 [==============================] - 2s 52ms/step - loss: 0.9495 - accuracy: 0.7020\n",
      "Epoch 238/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 0.9320 - accuracy: 0.7040\n",
      "Epoch 239/300\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.8973 - accuracy: 0.7210\n",
      "Epoch 240/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 0.8935 - accuracy: 0.7160\n",
      "Epoch 241/300\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.9072 - accuracy: 0.7190\n",
      "Epoch 242/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 57ms/step - loss: 0.9105 - accuracy: 0.7140\n",
      "Epoch 243/300\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.8803 - accuracy: 0.7180\n",
      "Epoch 244/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 0.8470 - accuracy: 0.7410\n",
      "Epoch 245/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 0.8842 - accuracy: 0.7240\n",
      "Epoch 246/300\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 1.0436 - accuracy: 0.6890\n",
      "Epoch 247/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 0.9775 - accuracy: 0.6950\n",
      "Epoch 248/300\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.9438 - accuracy: 0.7090\n",
      "Epoch 249/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.9289 - accuracy: 0.7110\n",
      "Epoch 250/300\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 1.0384 - accuracy: 0.6760\n",
      "Epoch 251/300\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 1.0448 - accuracy: 0.6870\n",
      "Epoch 252/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 1.1208 - accuracy: 0.6640\n",
      "Epoch 253/300\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 1.0055 - accuracy: 0.6860\n",
      "Epoch 254/300\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 0.9587 - accuracy: 0.6920\n",
      "Epoch 255/300\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 1.1041 - accuracy: 0.6700\n",
      "Epoch 256/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 1.2395 - accuracy: 0.6330\n",
      "Epoch 257/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 1.1566 - accuracy: 0.6640\n",
      "Epoch 258/300\n",
      "32/32 [==============================] - 2s 62ms/step - loss: 0.9648 - accuracy: 0.7070\n",
      "Epoch 259/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.9122 - accuracy: 0.7160\n",
      "Epoch 260/300\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.8816 - accuracy: 0.7300\n",
      "Epoch 261/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 0.8647 - accuracy: 0.7330\n",
      "Epoch 262/300\n",
      "32/32 [==============================] - 2s 52ms/step - loss: 0.8210 - accuracy: 0.7410\n",
      "Epoch 263/300\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.7970 - accuracy: 0.7500\n",
      "Epoch 264/300\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 0.8036 - accuracy: 0.7380\n",
      "Epoch 265/300\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 0.8160 - accuracy: 0.7530\n",
      "Epoch 266/300\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 0.7854 - accuracy: 0.7560\n",
      "Epoch 267/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.7784 - accuracy: 0.7470\n",
      "Epoch 268/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 0.7592 - accuracy: 0.7620\n",
      "Epoch 269/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 0.7534 - accuracy: 0.7560\n",
      "Epoch 270/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 0.7701 - accuracy: 0.7610\n",
      "Epoch 271/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 0.7495 - accuracy: 0.7630\n",
      "Epoch 272/300\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 0.7317 - accuracy: 0.7670\n",
      "Epoch 273/300\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 0.7723 - accuracy: 0.7610\n",
      "Epoch 274/300\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 0.8127 - accuracy: 0.7580\n",
      "Epoch 275/300\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 0.7900 - accuracy: 0.7540\n",
      "Epoch 276/300\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 0.8158 - accuracy: 0.7440\n",
      "Epoch 277/300\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.7938 - accuracy: 0.7500\n",
      "Epoch 278/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 0.7587 - accuracy: 0.7670\n",
      "Epoch 279/300\n",
      "32/32 [==============================] - 1s 43ms/step - loss: 0.8128 - accuracy: 0.7540\n",
      "Epoch 280/300\n",
      "32/32 [==============================] - 1s 44ms/step - loss: 0.7675 - accuracy: 0.7660\n",
      "Epoch 281/300\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 0.7833 - accuracy: 0.7530\n",
      "Epoch 282/300\n",
      "32/32 [==============================] - 1s 42ms/step - loss: 0.7624 - accuracy: 0.7620\n",
      "Epoch 283/300\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.7460 - accuracy: 0.7660\n",
      "Epoch 284/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 0.7495 - accuracy: 0.7640\n",
      "Epoch 285/300\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.7069 - accuracy: 0.7810\n",
      "Epoch 286/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 0.7257 - accuracy: 0.7820\n",
      "Epoch 287/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 0.7223 - accuracy: 0.7750\n",
      "Epoch 288/300\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 0.7214 - accuracy: 0.7670\n",
      "Epoch 289/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 0.7285 - accuracy: 0.7710\n",
      "Epoch 290/300\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.7990 - accuracy: 0.7480\n",
      "Epoch 291/300\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.8028 - accuracy: 0.7440\n",
      "Epoch 292/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 0.8242 - accuracy: 0.7360\n",
      "Epoch 293/300\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.8420 - accuracy: 0.7320\n",
      "Epoch 294/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.8009 - accuracy: 0.7360\n",
      "Epoch 295/300\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.8723 - accuracy: 0.7330\n",
      "Epoch 296/300\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 1.7564 - accuracy: 0.5680\n",
      "Epoch 297/300\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 1.6083 - accuracy: 0.5840\n",
      "Epoch 298/300\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 1.2792 - accuracy: 0.6290\n",
      "Epoch 299/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.9235 - accuracy: 0.7050\n",
      "Epoch 300/300\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 0.8494 - accuracy: 0.7370\n"
     ]
    }
   ],
   "source": [
    "xn=X.to_numpy()\n",
    "xn=xn.reshape(1000,28,28,1)\n",
    "\n",
    "yn=Y.to_numpy().reshape(1000,1)\n",
    "his=cnn.fit(xn,yn,epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting for test\n",
    "\n",
    "Xvt=df.drop(['Id','Label'],axis=1)\n",
    "xtt=Xvt.to_numpy()\n",
    "xtt=xtt[1000:1200]\n",
    "xtt=xtt.reshape(200,28,28,1)\n",
    "\n",
    "Yvt=pd.DataFrame(zip(df.Label))\n",
    "ytt=Yvt.to_numpy()\n",
    "ytt=ytt[1000:1200]\n",
    "ytt=ytt.reshape(200,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 18ms/step - loss: 19.3883 - accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[19.388324737548828, 0.10000000149011612]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.evaluate(xtt,ytt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9wvInAg4QqcZ",
    "outputId": "d4f0b2e4-44ed-42df-9f84-437742cee2c1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.compile(\n",
    "#     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) , #sparsecategorical\n",
    "# #     loss=tf.keras.losses.MeanSquaredError(),\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),metrics=['accuracy']\n",
    "# )\n",
    "# # optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# history = model.fit(\n",
    "#     X,Y,\n",
    "#     epochs=70\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "tPPUgXqSRcKa"
   },
   "outputs": [],
   "source": [
    "dft=pd.read_csv(\"C:\\\\Users\\\\redmi\\\\OneDrive\\\\Desktop\\\\INVICTA\\\\invicta-2022-level-2\\\\inv2_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "15fTAsahRl33",
    "outputId": "d48b5f9a-f958-4fd1-96b5-c28378909188"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132</td>\n",
       "      <td>205.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>...</td>\n",
       "      <td>112.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1546</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>424</td>\n",
       "      <td>58.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>66.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>380</td>\n",
       "      <td>224.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>...</td>\n",
       "      <td>196.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>767</td>\n",
       "      <td>224.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>...</td>\n",
       "      <td>218.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>1004</td>\n",
       "      <td>227.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>...</td>\n",
       "      <td>77.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>1687</td>\n",
       "      <td>123.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>141.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>210.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>576</td>\n",
       "      <td>233.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>...</td>\n",
       "      <td>103.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>534</td>\n",
       "      <td>22.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>411</td>\n",
       "      <td>61.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>562 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id      0      1      2      3      4      5      6      7      8  ...  \\\n",
       "0     132  205.0  237.0  234.0  170.0  128.0  172.0  209.0  182.0  168.0  ...   \n",
       "1    1546   30.0   30.0   32.0   29.0   31.0   34.0   31.0   28.0   28.0  ...   \n",
       "2     424   58.0   63.0   67.0   69.0   69.0   71.0   73.0   73.0   73.0  ...   \n",
       "3     380  224.0  223.0  224.0  224.0  224.0  224.0  224.0  224.0  225.0  ...   \n",
       "4     767  224.0  224.0  225.0  225.0  225.0  225.0  225.0  225.0  225.0  ...   \n",
       "..    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  ...   \n",
       "557  1004  227.0  228.0  230.0  230.0  236.0  241.0  242.0  237.0  234.0  ...   \n",
       "558  1687  123.0  139.0  125.0  151.0  136.0  141.0  114.0   65.0   70.0  ...   \n",
       "559   576  233.0  232.0  232.0  232.0  231.0  229.0  235.0  169.0  144.0  ...   \n",
       "560   534   22.0   13.0    0.0    1.0   32.0   30.0    0.0    1.0    1.0  ...   \n",
       "561   411   61.0   71.0   81.0   65.0   33.0   25.0   30.0   83.0  103.0  ...   \n",
       "\n",
       "       774    775    776    777    778    779    780    781    782    783  \n",
       "0    112.0   92.0  155.0  133.0   56.0   57.0   66.0   43.0   39.0   72.0  \n",
       "1     24.0   25.0   27.0   26.0   27.0   29.0   28.0   27.0   27.0   27.0  \n",
       "2     66.0  118.0  138.0  140.0  162.0  176.0  165.0  152.0  147.0  147.0  \n",
       "3    196.0  184.0  183.0  183.0  182.0  185.0  189.0  189.0  193.0  195.0  \n",
       "4    218.0  200.0  199.0  195.0  194.0  197.0  200.0  197.0  203.0  205.0  \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "557   77.0   75.0   96.0  146.0  177.0  191.0  203.0  211.0  212.0  221.0  \n",
       "558  141.0  137.0  183.0  241.0  223.0  165.0  154.0  148.0  159.0  210.0  \n",
       "559  103.0   78.0  115.0  153.0  198.0  214.0  219.0  220.0  219.0  221.0  \n",
       "560    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0  \n",
       "561  100.0   91.0  108.0   76.0   89.0  104.0  107.0  126.0  111.0  104.0  \n",
       "\n",
       "[562 rows x 785 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "N6Vkhri3XZHx"
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "oFH5UutDRyGv",
    "outputId": "de4ebbf1-01b6-4d66-82a4-6b5cbdc16b06"
   },
   "outputs": [],
   "source": [
    "dftest=dft.drop(['Id'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "9h80BP8RS81f"
   },
   "outputs": [],
   "source": [
    "# test1=dftest.loc[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest=dftest.to_numpy()\n",
    "xtest=xtest.reshape(562,28,28,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpsUjmA3RQFB",
    "outputId": "22712680-3132-4855-e491-2823fd6939fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "# prediction = model.predict(test1.reshape(1,784))\n",
    "prediction=cnn.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sX5-zifvUHH9",
    "outputId": "b90ec73e-ad01-41fe-d756-9e9621cce3fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.06400599e-07, 6.21304324e-08, 7.46269885e-04, ...,\n",
       "        7.62745971e-04, 1.00038555e-08, 3.01929703e-09],\n",
       "       [1.82287518e-09, 3.65639297e-09, 7.18770181e-08, ...,\n",
       "        2.87032500e-03, 2.30059374e-12, 3.18723353e-10],\n",
       "       [3.49841593e-03, 3.48785965e-11, 3.10723335e-05, ...,\n",
       "        5.52426167e-02, 3.98559230e-08, 2.94214464e-03],\n",
       "       ...,\n",
       "       [6.67347635e-08, 8.08426830e-06, 2.96297713e-07, ...,\n",
       "        9.68731911e-06, 1.91690363e-02, 9.07934368e-01],\n",
       "       [9.99794662e-01, 2.13385256e-12, 2.02849937e-09, ...,\n",
       "        8.08238254e-10, 8.33136511e-13, 8.25227566e-16],\n",
       "       [4.49453053e-09, 5.01580666e-10, 5.64879770e-07, ...,\n",
       "        1.08789289e-04, 2.21650826e-05, 6.58389823e-08]], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.argmax(prediction)\n",
    "prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.06400599e-07, 6.21304324e-08, 7.46269885e-04, 7.35290384e-10,\n",
       "       2.35762715e-10, 4.91802288e-09, 3.80362314e-03, 4.50574942e-02,\n",
       "       2.02432826e-09, 5.68693181e-08, 6.67797303e-06, 5.68608637e-04,\n",
       "       1.02501102e-02, 2.29263901e-06, 2.68691732e-03, 3.97814716e-07,\n",
       "       2.59234789e-09, 5.05821995e-10, 6.73358858e-01, 2.25308846e-04,\n",
       "       4.37427894e-04, 2.38211384e-09, 1.04103640e-06, 5.85975977e-06,\n",
       "       3.12798392e-08, 3.63474227e-02, 2.41595861e-02, 2.01577455e-01,\n",
       "       1.05886329e-06, 4.39536691e-08, 3.82604611e-07, 1.60602369e-07,\n",
       "       7.62745971e-04, 1.00038555e-08, 3.01929703e-09], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.82287518e-09, 3.65639297e-09, 7.18770181e-08, 1.78843045e-15,\n",
       "       1.01165005e-08, 8.93003289e-08, 8.18760881e-09, 7.87913138e-12,\n",
       "       1.26696614e-04, 1.04057840e-09, 3.74045417e-09, 5.92286941e-16,\n",
       "       4.38797478e-08, 1.53131102e-11, 7.27684948e-13, 3.88532698e-07,\n",
       "       1.24158427e-19, 2.68214673e-04, 1.26074639e-12, 4.12989718e-15,\n",
       "       1.16399137e-19, 1.13650012e-05, 2.42254959e-15, 2.00216036e-15,\n",
       "       1.43517864e-08, 1.93475413e-19, 2.48399527e-14, 1.67062670e-18,\n",
       "       9.92745697e-01, 4.93160945e-17, 4.93049438e-12, 3.97697045e-03,\n",
       "       2.87032500e-03, 2.30059374e-12, 3.18723353e-10], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "R_nqFyOtUNtM"
   },
   "outputs": [],
   "source": [
    "# data = {'Id':[],'Label':[]}\n",
    "# final=pd.DataFrame(data)\n",
    "# final\n",
    "# for i in range(0,534):\n",
    "#   testpic=dftest.loc[i].to_numpy()\n",
    "#   prediction = model.predict(test1.reshape(1,784))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "FeTimQ_-Y8iw"
   },
   "outputs": [],
   "source": [
    "list_predict_label=[]\n",
    "# list_predict_id=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VCJ4BbmVhLp",
    "outputId": "e057acbc-1ac0-4fae-d4a2-76a45b48e045"
   },
   "outputs": [],
   "source": [
    "for i in range(0,562):\n",
    "#   testpic=dftest.loc[i].to_numpy() #dftest\n",
    "#   prediction = model.predict()\n",
    "  ithpredict_label=prediction[i]\n",
    "  predict_label=np.argmax(ithpredict_label)\n",
    "#   predict_id=dft_label[0,i]\n",
    "  list_predict_label.append(predict_label)\n",
    "#   list_predict_id.append(predict_id)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "mroUnVPSZbtq",
    "outputId": "9819e50f-5884-4852-a0e1-dbcaec061dc7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1546</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>424</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>380</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>767</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>1004</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>1687</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>576</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>411</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>562 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0   1\n",
       "0     132  18\n",
       "1    1546  28\n",
       "2     424  12\n",
       "3     380  15\n",
       "4     767  28\n",
       "..    ...  ..\n",
       "557  1004  15\n",
       "558  1687  28\n",
       "559   576  34\n",
       "560   534   0\n",
       "561   411   7\n",
       "\n",
       "[562 rows x 2 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = {'Id':list_predict_id,'Label':list_predict_label}\n",
    "dft_id={'Label':list_predict_label}\n",
    "finaltemp=pd.DataFrame(dft_id)\n",
    "final=pd.DataFrame(zip(dft.Id,finaltemp.Label))\n",
    "final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1546</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>424</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>380</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>767</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>1004</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>1687</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>576</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>411</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>562 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id  Label\n",
       "0     132     18\n",
       "1    1546     28\n",
       "2     424     12\n",
       "3     380     15\n",
       "4     767     28\n",
       "..    ...    ...\n",
       "557  1004     15\n",
       "558  1687     28\n",
       "559   576     34\n",
       "560   534      0\n",
       "561   411      7\n",
       "\n",
       "[562 rows x 2 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dict = {'0': 'Id',\n",
    "#         '1': 'Label',\n",
    "#        }\n",
    " \n",
    "# # call rename () method\n",
    "# final.rename(columns=dict,\n",
    "#           inplace=True)\n",
    "\n",
    "# final.columns.values[0:2] =[\"Id\", \"Label\" ]\n",
    "\n",
    "final = final.rename({0: 'Id', 1: 'Label'}, axis=1)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28    93\n",
       "34    39\n",
       "18    31\n",
       "33    29\n",
       "15    25\n",
       "29    22\n",
       "10    21\n",
       "7     19\n",
       "31    18\n",
       "2     17\n",
       "25    17\n",
       "23    16\n",
       "32    15\n",
       "24    15\n",
       "13    14\n",
       "11    14\n",
       "14    14\n",
       "3     13\n",
       "6     12\n",
       "21    11\n",
       "16    11\n",
       "12    11\n",
       "8     10\n",
       "5      9\n",
       "0      8\n",
       "4      8\n",
       "27     8\n",
       "1      7\n",
       "20     6\n",
       "9      6\n",
       "19     6\n",
       "17     6\n",
       "26     5\n",
       "22     4\n",
       "30     2\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['Label'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"C:\\\\Users\\\\redmi\\\\OneDrive\\\\Desktop\\\\INVICTA\\\\invicta-2022-level-2\\\\results\\\\lvl2_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
